{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def telegram_bot_sendtext(bot_message):\n",
    "    \n",
    "    bot_token = '1153335989:AAE4v1w9FD_vCUaG2qcq-WmuPwh_MBYWWho'\n",
    "    bot_chatID = '675791133'\n",
    "    send_text = 'https://api.telegram.org/bot' + bot_token + '/sendMessage?chat_id=' + bot_chatID + '&parse_mode=Markdown&text=' + bot_message\n",
    "\n",
    "    response = requests.get(send_text)\n",
    "\n",
    "    bot_token_alan = '1181840666:AAFNcyJ_LWffRm3NU5f4hAack4ArKOsBwCw'\n",
    "    bot_chatID_alan = '1195073413'\n",
    "    send_text_alan = 'https://api.telegram.org/bot' + bot_token_alan + '/sendMessage?chat_id=' + bot_chatID_alan + '&parse_mode=Markdown&text=' + bot_message\n",
    "\n",
    "    response = requests.get(send_text_alan)\n",
    "\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# from memory_profiler import memory_usage\n",
    "# os - operating system allow us to This module provides a portable way of using operating system dependent functionality\n",
    "# It helps to have greater control over the interaction with file system\n",
    "import os \n",
    "import pandas as pd\n",
    "# In Python, the glob module is used to retrieve files/pathnames matching a specified pattern. The pattern rules of glob follow standard Unix path expansion rules. \n",
    "# Example:\n",
    "# # Using '?' pattern \n",
    "# print('\\nNamed with wildcard ?:') \n",
    "# for name in glob.glob('/home/geeks/Desktop/gfg/data?.txt'): \n",
    "#     print(name) \n",
    "from glob import glob\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/manoj/anaconda3/lib/python3.7/site-packages/librosa/util/decorators.py:9: NumbaDeprecationWarning: \u001b[1mAn import was requested from a module that has moved location.\n",
      "Import requested from: 'numba.decorators', please update to use 'numba.core.decorators' or pin to Numba version 0.48.0. This alias will not be present in Numba version 0.50.0.\u001b[0m\n",
      "  from numba.decorators import jit as optional_jit\n",
      "/home/manoj/anaconda3/lib/python3.7/site-packages/librosa/util/decorators.py:9: NumbaDeprecationWarning: \u001b[1mAn import was requested from a module that has moved location.\n",
      "Import of 'jit' requested from: 'numba.decorators', please update to use 'numba.core.decorators' or pin to Numba version 0.48.0. This alias will not be present in Numba version 0.50.0.\u001b[0m\n",
      "  from numba.decorators import jit as optional_jit\n"
     ]
    }
   ],
   "source": [
    "from keras import layers\n",
    "from keras import models\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.optimizers import Adam\n",
    "import keras.backend as K\n",
    "import librosa\n",
    "import librosa.display\n",
    "import pylab\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import figure\n",
    "# garbage collector\n",
    "import gc\n",
    "from path import Path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert wav file into images.\n",
    "We extract the audio time-series and sampling rate of each .wav file using LibROSA, before building and plotting a spectrogram of the data and saving it as a corresponding image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_spectrogram(filename, name):\n",
    "    plt.interactive(False)\n",
    "    clip, sample_rate = librosa.load(filename, sr=None)\n",
    "    fig = plt.figure(figsize=[0.72, 0.72])\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.axes.get_xaxis().set_visible(False)\n",
    "    ax.axes.get_yaxis().set_visible(False)\n",
    "    ax.set_frame_on(False)\n",
    "    S = librosa.feature.melspectrogram(y=clip,sr= sample_rate)\n",
    "    librosa.display.specshow(librosa.power_to_db(S, ref = np.max))\n",
    "    filename = '/home/manoj/HBRS/R_and_D/Github_folder/Researcha-and-development-project/Code/Tutorials/kaggle/working/train/'+name+'.jpg'\n",
    "    plt.savefig(filename, dpi=400,bbox_inches='tight',pad_inches=0)\n",
    "    plt.close()\n",
    "    fig.clf()\n",
    "    plt.close(fig)\n",
    "    plt.close('all')\n",
    "    del filename,name,clip,sample_rate,fig,ax,S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_spectrogram_test(filename,name):\n",
    "    plt.interactive(False)\n",
    "    clip, sample_rate = librosa.load(filename, sr=None)\n",
    "    fig = plt.figure(figsize=[0.72,0.72])\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.axes.get_xaxis().set_visible(False)\n",
    "    ax.axes.get_yaxis().set_visible(False)\n",
    "    ax.set_frame_on(False)\n",
    "    S = librosa.feature.melspectrogram(y=clip, sr=sample_rate)\n",
    "    librosa.display.specshow(librosa.power_to_db(S, ref=np.max))\n",
    "    filename  = Path('/home/manoj/HBRS/R_and_D/Github_folder/Researcha-and-development-project/Code/Tutorials/kaggle/working/test/' + name + '.jpg')\n",
    "    fig.savefig(filename, dpi=400, bbox_inches='tight',pad_inches=0)\n",
    "    plt.close()    \n",
    "    fig.clf()\n",
    "    plt.close(fig)\n",
    "    plt.close('all')\n",
    "    del filename,name,clip,sample_rate,fig,ax,S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_directory = np.array(glob(\"/home/manoj/HBRS/R_and_D/Github_folder/Researcha-and-development-project/Code/Tutorials/data/Train_data/Train/*\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/manoj/HBRS/R_and_D/Github_folder/Researcha-and-development-project/Code/Tutorials/data/Train_data/Train/6029.wav'\n",
      " '/home/manoj/HBRS/R_and_D/Github_folder/Researcha-and-development-project/Code/Tutorials/data/Train_data/Train/4089.wav'\n",
      " '/home/manoj/HBRS/R_and_D/Github_folder/Researcha-and-development-project/Code/Tutorials/data/Train_data/Train/3406.wav'\n",
      " ...\n",
      " '/home/manoj/HBRS/R_and_D/Github_folder/Researcha-and-development-project/Code/Tutorials/data/Train_data/Train/5504.wav'\n",
      " '/home/manoj/HBRS/R_and_D/Github_folder/Researcha-and-development-project/Code/Tutorials/data/Train_data/Train/5864.wav'\n",
      " '/home/manoj/HBRS/R_and_D/Github_folder/Researcha-and-development-project/Code/Tutorials/data/Train_data/Train/7955.wav']\n"
     ]
    }
   ],
   "source": [
    "print(data_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "for value, file in enumerate(data_directory):\n",
    "    filename,name = file,file.split('/')[-1].split('.')[0]\n",
    "    create_spectrogram(filename,name)\n",
    "    if value%50 == 0:\n",
    "        spectrogram = \"Generated \"+str(value)+\" spectrograms of audio signal.\"\n",
    "        telegram_bot_sendtext(spectrogram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_directory = np.array(glob(\"/home/manoj/HBRS/R_and_D/Github_folder/Researcha-and-development-project/Code/Tutorials/data/Test_data/Test/*\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/manoj/HBRS/R_and_D/Github_folder/Researcha-and-development-project/Code/Tutorials/data/Test_data/Test/3309.wav'\n",
      " '/home/manoj/HBRS/R_and_D/Github_folder/Researcha-and-development-project/Code/Tutorials/data/Test_data/Test/5849.wav'\n",
      " '/home/manoj/HBRS/R_and_D/Github_folder/Researcha-and-development-project/Code/Tutorials/data/Test_data/Test/4689.wav'\n",
      " ...\n",
      " '/home/manoj/HBRS/R_and_D/Github_folder/Researcha-and-development-project/Code/Tutorials/data/Test_data/Test/3139.wav'\n",
      " '/home/manoj/HBRS/R_and_D/Github_folder/Researcha-and-development-project/Code/Tutorials/data/Test_data/Test/8120.wav'\n",
      " '/home/manoj/HBRS/R_and_D/Github_folder/Researcha-and-development-project/Code/Tutorials/data/Test_data/Test/7758.wav']\n"
     ]
    }
   ],
   "source": [
    "print(test_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/manoj/anaconda3/lib/python3.7/site-packages/librosa/filters.py:235: UserWarning: Empty filters detected in mel frequency basis. Some channels will produce empty responses. Try increasing your sampling rate (and fmax) or reducing n_mels.\n",
      "  warnings.warn('Empty filters detected in mel frequency basis. '\n"
     ]
    }
   ],
   "source": [
    "for value, file in enumerate(test_directory):\n",
    "    filename,name = file,file.split('/')[-1].split('.')[0]\n",
    "    create_spectrogram_test(filename,name)\n",
    "    if value%50 == 0:\n",
    "        spectrogram_test = \"Generated \"+str(value)+\" spectrograms of test audio signal.\"\n",
    "        telegram_bot_sendtext(spectrogram_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ImageDataGenerator class\n",
    "\n",
    "* Generate batches of tensor image data with real-time data augmentation. The data will be looped over (in batches).\n",
    "\n",
    "# flow_from_dataframe method\n",
    "* Takes the dataframe and the path to a directory + generates batches. The generated batches contain augmented/normalized data.\n",
    "* Tutorial - https://medium.com/@vijayabhaskar96/tutorial-on-keras-imagedatagenerator-with-flow-from-dataframe-8bd5776e45c1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4077 validated image filenames belonging to 10 classes.\n",
      "Found 0 validated image filenames belonging to 0 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/manoj/anaconda3/lib/python3.7/site-packages/keras_preprocessing/image/dataframe_iterator.py:273: UserWarning: Found 5435 invalid image filename(s) in x_col=\"ID\". These filename(s) will be ignored.\n",
      "  .format(n_invalid, x_col)\n"
     ]
    }
   ],
   "source": [
    "def append_ext(fn):\n",
    "    return fn+\".jpg\"\n",
    "\n",
    "traindf = pd.read_csv(\"/home/manoj/HBRS/R_and_D/Github_folder/Researcha-and-development-project/Code/Tutorials/data/Train_data/train.csv\", dtype=str)\n",
    "testdf = pd.read_csv(\"/home/manoj/HBRS/R_and_D/Github_folder/Researcha-and-development-project/Code/Tutorials/data/Test_data/test.csv\", dtype=str)\n",
    "traindf[\"ID\"] = traindf[\"ID\"].apply(append_ext)\n",
    "testdf[\"ID\"] = testdf[\"ID\"].apply(append_ext)\n",
    "\n",
    "datagen = ImageDataGenerator(rescale=1./255, validation_split=0.25)\n",
    "\n",
    "train_generator = datagen.flow_from_dataframe(\n",
    "    dataframe=traindf,\n",
    "    directory=\"/home/manoj/HBRS/R_and_D/Github_folder/Researcha-and-development-project/Code/Tutorials/kaggle/working/train/\",\n",
    "    x_col=\"ID\",\n",
    "    y_col=\"Class\",\n",
    "    subset=\"training\",\n",
    "    batch_size=32,\n",
    "    seed=42,\n",
    "    shuffle=True,\n",
    "    class_mode=\"categorical\",\n",
    "    target_size=(64,64))\n",
    "\n",
    "valid_generator=datagen.flow_from_dataframe(\n",
    "    dataframe=traindf,\n",
    "    directory=\"/home/manoj/HBRS/R_and_D/Github_folder/Researcha-and-development-project/Code/Tutorials/kaggle/working/train/\",\n",
    "    x_col=\"ID\",\n",
    "    y_col=\"Class\",\n",
    "    subset=\"validation\",\n",
    "    batch_size=32,\n",
    "    seed=42,\n",
    "    shuffle=True,\n",
    "    class_mode=\"categorical\",\n",
    "    target_size=(64,64))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequential neural network model is built with the help of RMSProp optimizer.\n",
    "* The network consist of 6 convolutional layers with increased filter density in order to best extract the features of each image with each successive layer.\n",
    "* Pooling and dropout layers are used to increase computational efficiency and to prevent overfitting.\n",
    "* It is found that adding in layers with high filter number at the end of the network boosted accuracy by upto 3%.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_12 (Conv2D)           (None, 64, 64, 32)        896       \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 64, 64, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 62, 62, 64)        18496     \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 62, 62, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 31, 31, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 31, 31, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 31, 31, 64)        36928     \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 31, 31, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 29, 29, 64)        36928     \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 29, 29, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_16 (Conv2D)           (None, 14, 14, 128)       73856     \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 12, 12, 128)       147584    \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 12, 12, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 4608)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 512)               2359808   \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 2,679,626\n",
      "Trainable params: 2,679,626\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dense, Activation, Flatten, Dropout, BatchNormalization\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import regularizers, optimizers\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), padding='same',\n",
    "                 input_shape=(64,64,3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Conv2D(128, (3, 3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(128, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "model.compile(optimizers.RMSprop(lr=0.0005, decay=1e-6),loss=\"categorical_crossentropy\",metrics=[\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting and evaluating the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "127/127 [==============================] - 109s 855ms/step - loss: 2.0031 - accuracy: 0.2653\n",
      "Epoch 2/150\n",
      "127/127 [==============================] - 95s 745ms/step - loss: 1.5891 - accuracy: 0.4541\n",
      "Epoch 3/150\n",
      "127/127 [==============================] - 97s 762ms/step - loss: 1.2635 - accuracy: 0.5671\n",
      "Epoch 4/150\n",
      "127/127 [==============================] - 115s 908ms/step - loss: 1.0498 - accuracy: 0.6494\n",
      "Epoch 5/150\n",
      "127/127 [==============================] - 155s 1s/step - loss: 0.8825 - accuracy: 0.7006\n",
      "Epoch 6/150\n",
      "127/127 [==============================] - 148s 1s/step - loss: 0.7503 - accuracy: 0.7555\n",
      "Epoch 7/150\n",
      "127/127 [==============================] - 150s 1s/step - loss: 0.6650 - accuracy: 0.7792\n",
      "Epoch 8/150\n",
      "127/127 [==============================] - 114s 898ms/step - loss: 0.5723 - accuracy: 0.8193\n",
      "Epoch 9/150\n",
      "127/127 [==============================] - 98s 774ms/step - loss: 0.5008 - accuracy: 0.8354\n",
      "Epoch 10/150\n",
      "127/127 [==============================] - 96s 759ms/step - loss: 0.4682 - accuracy: 0.8443\n",
      "Epoch 11/150\n",
      "127/127 [==============================] - 97s 763ms/step - loss: 0.4048 - accuracy: 0.8621\n",
      "Epoch 12/150\n",
      "127/127 [==============================] - 97s 764ms/step - loss: 0.3760 - accuracy: 0.8700\n",
      "Epoch 13/150\n",
      "127/127 [==============================] - 95s 750ms/step - loss: 0.3255 - accuracy: 0.8917\n",
      "Epoch 14/150\n",
      "127/127 [==============================] - 100s 787ms/step - loss: 0.2978 - accuracy: 0.8996\n",
      "Epoch 15/150\n",
      "127/127 [==============================] - 96s 759ms/step - loss: 0.2695 - accuracy: 0.9140\n",
      "Epoch 16/150\n",
      "127/127 [==============================] - 96s 759ms/step - loss: 0.2632 - accuracy: 0.9108\n",
      "Epoch 17/150\n",
      "127/127 [==============================] - 98s 773ms/step - loss: 0.2367 - accuracy: 0.9189\n",
      "Epoch 18/150\n",
      "127/127 [==============================] - 100s 785ms/step - loss: 0.2402 - accuracy: 0.9216\n",
      "Epoch 19/150\n",
      "127/127 [==============================] - 94s 739ms/step - loss: 0.2074 - accuracy: 0.9308\n",
      "Epoch 20/150\n",
      "127/127 [==============================] - 94s 739ms/step - loss: 0.1987 - accuracy: 0.9293\n",
      "Epoch 21/150\n",
      "127/127 [==============================] - 106s 833ms/step - loss: 0.1735 - accuracy: 0.9409\n",
      "Epoch 22/150\n",
      "127/127 [==============================] - 134s 1s/step - loss: 0.1692 - accuracy: 0.9426\n",
      "Epoch 23/150\n",
      "127/127 [==============================] - 141s 1s/step - loss: 0.1540 - accuracy: 0.9476\n",
      "Epoch 24/150\n",
      "127/127 [==============================] - 121s 949ms/step - loss: 0.1611 - accuracy: 0.9439\n",
      "Epoch 25/150\n",
      "127/127 [==============================] - 109s 857ms/step - loss: 0.1479 - accuracy: 0.9511\n",
      "Epoch 26/150\n",
      "127/127 [==============================] - 96s 759ms/step - loss: 0.1429 - accuracy: 0.9513\n",
      "Epoch 27/150\n",
      "127/127 [==============================] - 106s 833ms/step - loss: 0.1358 - accuracy: 0.9560\n",
      "Epoch 28/150\n",
      "127/127 [==============================] - 92s 728ms/step - loss: 0.1330 - accuracy: 0.9530\n",
      "Epoch 29/150\n",
      "127/127 [==============================] - 119s 935ms/step - loss: 0.1242 - accuracy: 0.9609\n",
      "Epoch 30/150\n",
      "127/127 [==============================] - 97s 761ms/step - loss: 0.1131 - accuracy: 0.9622\n",
      "Epoch 31/150\n",
      "127/127 [==============================] - 97s 762ms/step - loss: 0.1239 - accuracy: 0.9609\n",
      "Epoch 32/150\n",
      "127/127 [==============================] - 134s 1s/step - loss: 0.1089 - accuracy: 0.9607\n",
      "Epoch 33/150\n",
      "127/127 [==============================] - 122s 957ms/step - loss: 0.1086 - accuracy: 0.9646\n",
      "Epoch 34/150\n",
      "127/127 [==============================] - 104s 818ms/step - loss: 0.1167 - accuracy: 0.9637\n",
      "Epoch 35/150\n",
      "127/127 [==============================] - 94s 740ms/step - loss: 0.1167 - accuracy: 0.9639\n",
      "Epoch 36/150\n",
      "127/127 [==============================] - 93s 734ms/step - loss: 0.0950 - accuracy: 0.9674\n",
      "Epoch 37/150\n",
      "127/127 [==============================] - 97s 764ms/step - loss: 0.1131 - accuracy: 0.9656\n",
      "Epoch 38/150\n",
      "127/127 [==============================] - 90s 710ms/step - loss: 0.1017 - accuracy: 0.9646\n",
      "Epoch 39/150\n",
      "127/127 [==============================] - 107s 839ms/step - loss: 0.1033 - accuracy: 0.9742\n",
      "Epoch 40/150\n",
      "127/127 [==============================] - 145s 1s/step - loss: 0.0929 - accuracy: 0.9731\n",
      "Epoch 41/150\n",
      "127/127 [==============================] - 133s 1s/step - loss: 0.0876 - accuracy: 0.9718\n",
      "Epoch 42/150\n",
      "127/127 [==============================] - 115s 902ms/step - loss: 0.0973 - accuracy: 0.9728\n",
      "Epoch 43/150\n",
      "127/127 [==============================] - 117s 922ms/step - loss: 0.0984 - accuracy: 0.9693\n",
      "Epoch 44/150\n",
      "127/127 [==============================] - 112s 886ms/step - loss: 0.0958 - accuracy: 0.9728\n",
      "Epoch 45/150\n",
      "127/127 [==============================] - 121s 950ms/step - loss: 0.0804 - accuracy: 0.9740\n",
      "Epoch 46/150\n",
      "127/127 [==============================] - 115s 906ms/step - loss: 0.0867 - accuracy: 0.9721\n",
      "Epoch 47/150\n",
      "127/127 [==============================] - 115s 904ms/step - loss: 0.0932 - accuracy: 0.9728\n",
      "Epoch 48/150\n",
      "127/127 [==============================] - 116s 917ms/step - loss: 0.0754 - accuracy: 0.9755\n",
      "Epoch 49/150\n",
      "127/127 [==============================] - 142s 1s/step - loss: 0.0897 - accuracy: 0.9743\n",
      "Epoch 50/150\n",
      "127/127 [==============================] - 146s 1s/step - loss: 0.0911 - accuracy: 0.9763\n",
      "Epoch 51/150\n",
      "127/127 [==============================] - 119s 939ms/step - loss: 0.0995 - accuracy: 0.9718\n",
      "Epoch 52/150\n",
      "127/127 [==============================] - 138s 1s/step - loss: 0.0824 - accuracy: 0.9770\n",
      "Epoch 53/150\n",
      "127/127 [==============================] - 147s 1s/step - loss: 0.0898 - accuracy: 0.9760\n",
      "Epoch 54/150\n",
      "127/127 [==============================] - 162s 1s/step - loss: 0.1004 - accuracy: 0.9718\n",
      "Epoch 55/150\n",
      "127/127 [==============================] - 159s 1s/step - loss: 0.0948 - accuracy: 0.9753\n",
      "Epoch 56/150\n",
      "127/127 [==============================] - 158s 1s/step - loss: 0.0912 - accuracy: 0.9740\n",
      "Epoch 57/150\n",
      "127/127 [==============================] - 166s 1s/step - loss: 0.0980 - accuracy: 0.9755\n",
      "Epoch 58/150\n",
      "127/127 [==============================] - 159s 1s/step - loss: 0.1039 - accuracy: 0.9740\n",
      "Epoch 59/150\n",
      "127/127 [==============================] - 156s 1s/step - loss: 0.1026 - accuracy: 0.9760\n",
      "Epoch 60/150\n",
      "127/127 [==============================] - 158s 1s/step - loss: 0.0958 - accuracy: 0.9731\n",
      "Epoch 61/150\n",
      "127/127 [==============================] - 154s 1s/step - loss: 0.0869 - accuracy: 0.9782\n",
      "Epoch 62/150\n",
      "127/127 [==============================] - 150s 1s/step - loss: 0.1092 - accuracy: 0.9721\n",
      "Epoch 63/150\n",
      "127/127 [==============================] - 114s 896ms/step - loss: 0.1098 - accuracy: 0.9765\n",
      "Epoch 64/150\n",
      "127/127 [==============================] - 112s 884ms/step - loss: 0.1107 - accuracy: 0.9723\n",
      "Epoch 65/150\n",
      "127/127 [==============================] - 113s 890ms/step - loss: 0.1009 - accuracy: 0.9755\n",
      "Epoch 66/150\n",
      "127/127 [==============================] - 114s 897ms/step - loss: 0.0896 - accuracy: 0.9755\n",
      "Epoch 67/150\n",
      "127/127 [==============================] - 113s 891ms/step - loss: 0.1105 - accuracy: 0.9760\n",
      "Epoch 68/150\n",
      "127/127 [==============================] - 114s 901ms/step - loss: 0.0997 - accuracy: 0.9755\n",
      "Epoch 69/150\n",
      "127/127 [==============================] - 114s 895ms/step - loss: 0.1106 - accuracy: 0.9738\n",
      "Epoch 70/150\n",
      "127/127 [==============================] - 113s 891ms/step - loss: 0.1158 - accuracy: 0.9701\n",
      "Epoch 71/150\n",
      "127/127 [==============================] - 115s 903ms/step - loss: 0.1157 - accuracy: 0.9693\n",
      "Epoch 72/150\n",
      "127/127 [==============================] - 161s 1s/step - loss: 0.0916 - accuracy: 0.9758\n",
      "Epoch 73/150\n",
      "127/127 [==============================] - 120s 947ms/step - loss: 0.0710 - accuracy: 0.9807\n",
      "Epoch 74/150\n",
      "127/127 [==============================] - 120s 941ms/step - loss: 0.1153 - accuracy: 0.9713\n",
      "Epoch 75/150\n",
      "127/127 [==============================] - 122s 959ms/step - loss: 0.0920 - accuracy: 0.9775\n",
      "Epoch 76/150\n",
      "127/127 [==============================] - 116s 916ms/step - loss: 0.1303 - accuracy: 0.9701\n",
      "Epoch 77/150\n",
      "127/127 [==============================] - 116s 915ms/step - loss: 0.0852 - accuracy: 0.9805\n",
      "Epoch 78/150\n",
      "127/127 [==============================] - 115s 909ms/step - loss: 0.0940 - accuracy: 0.9765\n",
      "Epoch 79/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "127/127 [==============================] - 116s 912ms/step - loss: 0.0949 - accuracy: 0.9785\n",
      "Epoch 80/150\n",
      "127/127 [==============================] - 116s 915ms/step - loss: 0.0949 - accuracy: 0.9765\n",
      "Epoch 81/150\n",
      "127/127 [==============================] - 117s 922ms/step - loss: 0.1095 - accuracy: 0.9728\n",
      "Epoch 82/150\n",
      "127/127 [==============================] - 116s 910ms/step - loss: 0.1189 - accuracy: 0.9735\n",
      "Epoch 83/150\n",
      "127/127 [==============================] - 115s 906ms/step - loss: 0.1188 - accuracy: 0.9738\n",
      "Epoch 84/150\n",
      "127/127 [==============================] - 117s 919ms/step - loss: 0.1104 - accuracy: 0.9765\n",
      "Epoch 85/150\n",
      "127/127 [==============================] - 120s 946ms/step - loss: 0.0972 - accuracy: 0.9753\n",
      "Epoch 86/150\n",
      "127/127 [==============================] - 116s 914ms/step - loss: 0.1207 - accuracy: 0.9740\n",
      "Epoch 87/150\n",
      "127/127 [==============================] - 116s 916ms/step - loss: 0.1126 - accuracy: 0.9740\n",
      "Epoch 88/150\n",
      "127/127 [==============================] - 119s 936ms/step - loss: 0.0853 - accuracy: 0.9763\n",
      "Epoch 89/150\n",
      "127/127 [==============================] - 116s 914ms/step - loss: 0.1091 - accuracy: 0.9748\n",
      "Epoch 90/150\n",
      "127/127 [==============================] - 115s 908ms/step - loss: 0.1086 - accuracy: 0.9765\n",
      "Epoch 91/150\n",
      "127/127 [==============================] - 115s 907ms/step - loss: 0.0999 - accuracy: 0.9748\n",
      "Epoch 92/150\n",
      "127/127 [==============================] - 116s 915ms/step - loss: 0.0969 - accuracy: 0.9768\n",
      "Epoch 93/150\n",
      "127/127 [==============================] - 119s 934ms/step - loss: 0.1174 - accuracy: 0.9768\n",
      "Epoch 94/150\n",
      "127/127 [==============================] - 117s 918ms/step - loss: 0.1005 - accuracy: 0.9750\n",
      "Epoch 95/150\n",
      "127/127 [==============================] - 117s 918ms/step - loss: 0.1200 - accuracy: 0.9738\n",
      "Epoch 96/150\n",
      "127/127 [==============================] - 115s 908ms/step - loss: 0.1214 - accuracy: 0.9728\n",
      "Epoch 97/150\n",
      "127/127 [==============================] - 140s 1s/step - loss: 0.1188 - accuracy: 0.9726\n",
      "Epoch 98/150\n",
      "127/127 [==============================] - 149s 1s/step - loss: 0.1120 - accuracy: 0.9765\n",
      "Epoch 99/150\n",
      "127/127 [==============================] - 93s 734ms/step - loss: 0.1187 - accuracy: 0.9765\n",
      "Epoch 100/150\n",
      "127/127 [==============================] - 94s 740ms/step - loss: 0.1082 - accuracy: 0.9738\n",
      "Epoch 101/150\n",
      "127/127 [==============================] - 94s 741ms/step - loss: 0.1516 - accuracy: 0.9735\n",
      "Epoch 102/150\n",
      "127/127 [==============================] - 93s 736ms/step - loss: 0.1152 - accuracy: 0.9745\n",
      "Epoch 103/150\n",
      "127/127 [==============================] - 102s 804ms/step - loss: 0.1441 - accuracy: 0.9755\n",
      "Epoch 104/150\n",
      "127/127 [==============================] - 93s 729ms/step - loss: 0.1014 - accuracy: 0.9768\n",
      "Epoch 105/150\n",
      "127/127 [==============================] - 107s 841ms/step - loss: 0.1505 - accuracy: 0.9728\n",
      "Epoch 106/150\n",
      "127/127 [==============================] - 113s 889ms/step - loss: 0.1190 - accuracy: 0.9731\n",
      "Epoch 107/150\n",
      "127/127 [==============================] - 95s 745ms/step - loss: 0.1456 - accuracy: 0.9731\n",
      "Epoch 108/150\n",
      "127/127 [==============================] - 107s 840ms/step - loss: 0.1259 - accuracy: 0.9728\n",
      "Epoch 109/150\n",
      "127/127 [==============================] - 113s 886ms/step - loss: 0.1290 - accuracy: 0.9728\n",
      "Epoch 110/150\n",
      "127/127 [==============================] - 115s 907ms/step - loss: 0.1239 - accuracy: 0.9731\n",
      "Epoch 111/150\n",
      "127/127 [==============================] - 124s 975ms/step - loss: 0.1428 - accuracy: 0.9716\n",
      "Epoch 112/150\n",
      "127/127 [==============================] - 175s 1s/step - loss: 0.1120 - accuracy: 0.9750\n",
      "Epoch 113/150\n",
      "127/127 [==============================] - 214s 2s/step - loss: 0.1280 - accuracy: 0.9750\n",
      "Epoch 114/150\n",
      "127/127 [==============================] - 216s 2s/step - loss: 0.1318 - accuracy: 0.9758\n",
      "Epoch 115/150\n",
      "127/127 [==============================] - 202s 2s/step - loss: 0.1585 - accuracy: 0.9654\n",
      "Epoch 116/150\n",
      "127/127 [==============================] - 170s 1s/step - loss: 0.1424 - accuracy: 0.9693\n",
      "Epoch 117/150\n",
      "127/127 [==============================] - 170s 1s/step - loss: 0.1162 - accuracy: 0.9770\n",
      "Epoch 118/150\n",
      "127/127 [==============================] - 169s 1s/step - loss: 0.1329 - accuracy: 0.9748\n",
      "Epoch 119/150\n",
      "127/127 [==============================] - 170s 1s/step - loss: 0.1087 - accuracy: 0.9773\n",
      "Epoch 120/150\n",
      "127/127 [==============================] - 176s 1s/step - loss: 0.1411 - accuracy: 0.9738\n",
      "Epoch 121/150\n",
      "127/127 [==============================] - 231s 2s/step - loss: 0.1522 - accuracy: 0.9723\n",
      "Epoch 122/150\n",
      "127/127 [==============================] - 175s 1s/step - loss: 0.1297 - accuracy: 0.9738\n",
      "Epoch 123/150\n",
      "127/127 [==============================] - 187s 1s/step - loss: 0.1687 - accuracy: 0.9689\n",
      "Epoch 124/150\n",
      "127/127 [==============================] - 94s 739ms/step - loss: 0.1724 - accuracy: 0.9740\n",
      "Epoch 125/150\n",
      "127/127 [==============================] - 94s 744ms/step - loss: 0.1496 - accuracy: 0.9711\n",
      "Epoch 126/150\n",
      "127/127 [==============================] - 94s 737ms/step - loss: 0.1549 - accuracy: 0.9654\n",
      "Epoch 127/150\n",
      "127/127 [==============================] - 97s 760ms/step - loss: 0.1495 - accuracy: 0.9745\n",
      "Epoch 128/150\n",
      "127/127 [==============================] - 90s 712ms/step - loss: 0.1562 - accuracy: 0.9696\n",
      "Epoch 129/150\n",
      "127/127 [==============================] - 91s 716ms/step - loss: 0.1228 - accuracy: 0.9726\n",
      "Epoch 130/150\n",
      "127/127 [==============================] - 91s 713ms/step - loss: 0.1346 - accuracy: 0.9740\n",
      "Epoch 131/150\n",
      "127/127 [==============================] - 90s 712ms/step - loss: 0.1574 - accuracy: 0.9701\n",
      "Epoch 132/150\n",
      "127/127 [==============================] - 90s 710ms/step - loss: 0.1682 - accuracy: 0.9686\n",
      "Epoch 133/150\n",
      "127/127 [==============================] - 90s 710ms/step - loss: 0.1547 - accuracy: 0.9733\n",
      "Epoch 134/150\n",
      "127/127 [==============================] - 90s 711ms/step - loss: 0.1673 - accuracy: 0.9674\n",
      "Epoch 135/150\n",
      "127/127 [==============================] - 90s 711ms/step - loss: 0.1531 - accuracy: 0.9753\n",
      "Epoch 136/150\n",
      "127/127 [==============================] - 91s 714ms/step - loss: 0.1321 - accuracy: 0.9726\n",
      "Epoch 137/150\n",
      "127/127 [==============================] - 90s 712ms/step - loss: 0.1660 - accuracy: 0.9708\n",
      "Epoch 138/150\n",
      "127/127 [==============================] - 90s 710ms/step - loss: 0.1974 - accuracy: 0.9691\n",
      "Epoch 139/150\n",
      "127/127 [==============================] - 90s 708ms/step - loss: 0.1828 - accuracy: 0.9713\n",
      "Epoch 140/150\n",
      "127/127 [==============================] - 90s 711ms/step - loss: 0.1578 - accuracy: 0.9686\n",
      "Epoch 141/150\n",
      "127/127 [==============================] - 90s 711ms/step - loss: 0.1889 - accuracy: 0.9716\n",
      "Epoch 142/150\n",
      "127/127 [==============================] - 91s 719ms/step - loss: 0.1854 - accuracy: 0.9698\n",
      "Epoch 143/150\n",
      "127/127 [==============================] - 90s 710ms/step - loss: 0.1882 - accuracy: 0.9693\n",
      "Epoch 144/150\n",
      "127/127 [==============================] - 90s 712ms/step - loss: 0.1712 - accuracy: 0.9674\n",
      "Epoch 145/150\n",
      "127/127 [==============================] - 96s 755ms/step - loss: 0.1707 - accuracy: 0.9659\n",
      "Epoch 146/150\n",
      "127/127 [==============================] - 90s 711ms/step - loss: 0.1493 - accuracy: 0.9733\n",
      "Epoch 147/150\n",
      "127/127 [==============================] - 91s 717ms/step - loss: 0.1477 - accuracy: 0.9723\n",
      "Epoch 148/150\n",
      "127/127 [==============================] - 91s 716ms/step - loss: 0.1640 - accuracy: 0.9718\n",
      "Epoch 149/150\n",
      "127/127 [==============================] - 91s 719ms/step - loss: 0.1692 - accuracy: 0.9661\n",
      "Epoch 150/150\n",
      "127/127 [==============================] - 91s 715ms/step - loss: 0.1729 - accuracy: 0.9740\n",
      "WARNING:tensorflow:From <ipython-input-47-e28ce7d82244>:10: Model.evaluate_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use Model.evaluate, which supports generators.\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'logs' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-e28ce7d82244>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m                     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m150\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m )\n\u001b[0;32m---> 10\u001b[0;31m model.evaluate_generator(generator=valid_generator, steps=STEP_SIZE_VALID\n\u001b[0m\u001b[1;32m     11\u001b[0m )\n",
      "\u001b[0;32m/home/manoj/anaconda3/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m               \u001b[0;34m'in a future version'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'after %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m               instructions)\n\u001b[0;32m--> 324\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m     return tf_decorator.make_decorator(\n\u001b[1;32m    326\u001b[0m         \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'deprecated'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/manoj/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate_generator\u001b[0;34m(self, generator, steps, callbacks, max_queue_size, workers, use_multiprocessing, verbose)\u001b[0m\n\u001b[1;32m   1505\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1506\u001b[0m         \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1507\u001b[0;31m         callbacks=callbacks)\n\u001b[0m\u001b[1;32m   1508\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1509\u001b[0m   @deprecation.deprecated(\n",
      "\u001b[0;32m/home/manoj/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/manoj/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict)\u001b[0m\n\u001b[1;32m   1089\u001b[0m       \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_test_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1090\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1091\u001b[0;31m       \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1092\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1093\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'logs' referenced before assignment"
     ]
    }
   ],
   "source": [
    "STEP_SIZE_TRAIN=train_generator.n//train_generator.batch_size\n",
    "STEP_SIZE_VALID=valid_generator.n//valid_generator.batch_size\n",
    "#STEP_SIZE_TEST=test_generator.n//test_generator.batch_size\n",
    "model.fit_generator(generator=train_generator,\n",
    "                    steps_per_epoch=STEP_SIZE_TRAIN,\n",
    "                    validation_data=valid_generator,\n",
    "                    validation_steps=STEP_SIZE_VALID,\n",
    "                    epochs=150\n",
    ")\n",
    "model.evaluate_generator(generator=valid_generator, steps=STEP_SIZE_VALID\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3297 validated image filenames.\n"
     ]
    }
   ],
   "source": [
    "test_datagen=ImageDataGenerator(rescale=1./255.)\n",
    "test_generator=test_datagen.flow_from_dataframe(\n",
    "    dataframe=testdf,\n",
    "    directory=\"/home/manoj/HBRS/R_and_D/Github_folder/Researcha-and-development-project/Code/Tutorials/kaggle/working/test/\",\n",
    "    x_col=\"ID\",\n",
    "    y_col=None,\n",
    "    batch_size=32,\n",
    "    seed=42,\n",
    "    shuffle=False,\n",
    "    class_mode=None,\n",
    "    target_size=(64,64))\n",
    "STEP_SIZE_TEST=test_generator.n//test_generator.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-49-3f838fb131a3>:4: Model.predict_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use Model.predict, which supports generators.\n",
      "103/103 [==============================] - 19s 183ms/step\n",
      "['jackhammer', 'dog_bark', 'drilling', 'dog_bark', 'street_music', 'jackhammer']\n"
     ]
    }
   ],
   "source": [
    "test_generator.reset()\n",
    "pred=model.predict_generator(test_generator,\n",
    "steps=STEP_SIZE_TEST,\n",
    "verbose=1)\n",
    "predicted_class_indices=np.argmax(pred,axis=1)\n",
    "\n",
    "#Fetch labels from train gen for testing\n",
    "labels = (train_generator.class_indices)\n",
    "labels = dict((v,k) for k,v in labels.items())\n",
    "predictions = [labels[k] for k in predicted_class_indices]\n",
    "print(predictions[0:6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resource \n",
    "\n",
    "* https://medium.com/gradientcrescent/urban-sound-classification-using-convolutional-neural-networks-with-keras-theory-and-486e92785df4\n",
    "* https://www.kaggle.com/devilsknight/sound-classification-using-spectrogram-images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
